{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caminhos e pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 200\n",
    "\n",
    "import os\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf  \n",
    "from pydub import AudioSegment\n",
    "import math  \n",
    "import shutil\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "import scipy\n",
    "from IPython.display import Audio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "###################################################################\n",
    "#Para converter mp3 em wav\n",
    "from pydub import AudioSegment\n",
    "AudioSegment.converter = \"C:\\\\ffmpeg\\\\bin\\\\ffmpeg.exe\"\n",
    "AudioSegment.ffmpeg = \"C:\\\\ffmpeg\\\\bin\\\\ffmpeg.exe\"\n",
    "AudioSegment.ffprobe =\"C:\\\\ffmpeg\\\\bin\\\\ffprobe.exe\"\n",
    "###################################################################\n",
    "\n",
    "\n",
    "#Para montar MFCCs e Filter Banks\n",
    "from python_speech_features import mfcc\n",
    "from python_speech_features import logfbank\n",
    "\n",
    "from time import gmtime, strftime\n",
    "\n",
    "#Folder Inicial\n",
    "path = os.getcwd()\n",
    "pathin = path + '\\\\entrada\\\\'   \n",
    "pathparcial = path + '\\\\parcial\\\\'\n",
    "pathaux = path + '\\\\auxiliar\\\\'\n",
    "pathout = path + '\\\\saida\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lendo Base de Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00,  8.98it/s]\n"
     ]
    }
   ],
   "source": [
    "name = 'Teste'\n",
    "df = joblib.load(pathparcial + name + 'V2.pkl')\n",
    "df.reset_index(inplace = True)\n",
    "\n",
    "df['mfccMean'] = np.nan\n",
    "df['mfccMean'] = df['mfccMean'].astype(object)\n",
    "\n",
    "for row in tqdm(range(0, df.shape[0])):  \n",
    "    mfcc_feat = mfcc(df['signal'][row], df['rate'][row]) #Array de multiplas linhas e 13 colunas\n",
    "    df.at[row, 'mfccMean'] = mfcc_feat.mean(axis=0) #Media por coluna\n",
    "    \n",
    "teste = df.drop(columns = ['signal'])\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 945/945 [01:15<00:00, 12.47it/s]\n"
     ]
    }
   ],
   "source": [
    "name = 'Train'\n",
    "df = joblib.load(pathparcial + name + 'V2.pkl')\n",
    "df.reset_index(inplace = True)\n",
    "\n",
    "df['mfccMean'] = np.nan\n",
    "df['mfccMean'] = df['mfccMean'].astype(object)\n",
    "\n",
    "for row in tqdm(range(0, df.shape[0])):  \n",
    "    mfcc_feat = mfcc(df['signal'][row], df['rate'][row]) #Array de multiplas linhas e 13 colunas\n",
    "    df.at[row, 'mfccMean'] = mfcc_feat.mean(axis=0) #Media por coluna\n",
    "    \n",
    "train = df.drop(columns = ['signal'])\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train0 = train.copy()\n",
    "teste0 = teste.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construindo o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.utils import parallel_backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo 1: Genero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 'file'\n",
    "id2 = 'cantor'\n",
    "y = 'masculino'\n",
    "x = 'mfccMean'\n",
    "\n",
    "train = train[[id, id2, y, x]]\n",
    "teste = teste[[id, id2, y, x]]\n",
    "\n",
    "#Treino\n",
    "trainMFCC = pd.DataFrame(train[x].values.tolist(),\n",
    "                         columns=['mfcc1', 'mfcc2', 'mfcc3', 'mfcc4', 'mfcc5', 'mfcc6', 'mfcc7', 'mfcc8', \n",
    "                                  'mfcc9', 'mfcc10', 'mfcc11', 'mfcc12', 'mfcc13'])\n",
    "train = pd.concat([train, trainMFCC], axis=1)\n",
    "train.drop(columns = [x], inplace = True)\n",
    "\n",
    "#Teste\n",
    "testeMFCC = pd.DataFrame(teste[x].values.tolist(),\n",
    "                         columns=['mfcc1', 'mfcc2', 'mfcc3', 'mfcc4', 'mfcc5', 'mfcc6', 'mfcc7', 'mfcc8', \n",
    "                                  'mfcc9', 'mfcc10', 'mfcc11', 'mfcc12', 'mfcc13'])\n",
    "teste = pd.concat([teste, testeMFCC], axis=1)\n",
    "teste.drop(columns = [x], inplace = True)\n",
    "\n",
    "varsmod = ['mfcc1', 'mfcc2', 'mfcc3', 'mfcc4', 'mfcc5', 'mfcc6', 'mfcc7', 'mfcc8', \n",
    "                                  'mfcc9', 'mfcc10', 'mfcc11', 'mfcc12', 'mfcc13']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nomemodelo = 'Logistica'\n",
    "\n",
    "seed = 123\n",
    "\n",
    "parameters = {\n",
    "    #'max_iter': [5000],\n",
    "    'C': [.01, .1, 1],\n",
    "    'penalty': ['l1'],\n",
    "    'fit_intercept': [True],\n",
    "    'class_weight': ['balanced', None],\n",
    "    #'clf__solver': ['lbfgs'],\n",
    "    'random_state': [seed]\n",
    "}\n",
    "\n",
    "estimator = LogisticRegression()\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits = 3, n_repeats = 1, random_state = seed)\n",
    "\n",
    "gs = GridSearchCV(estimator = estimator, param_grid = parameters, cv = cv,\n",
    "                  scoring = 'roc_auc', n_jobs = 3, verbose = 1, refit = True, return_train_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-19 03:39:55\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend MultiprocessingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  18 out of  18 | elapsed:    4.7s finished\n",
      "C:\\Users\\laran\\Anaconda3\\envs\\transc\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-19 03:40:00\n"
     ]
    }
   ],
   "source": [
    "#Rodando GridSearch\n",
    "print(strftime('%Y-%m-%d %H:%M:%S', gmtime()))\n",
    "\n",
    "with parallel_backend('multiprocessing'):\n",
    "    gs.fit(train[varsmod], train[y])\n",
    "\n",
    "joblib.dump(gs, pathaux + 'Modelo ' + nomemodelo + '.pkl')\n",
    "\n",
    "print(strftime('%Y-%m-%d %H:%M:%S', gmtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9719518785599636"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1,\n",
       " 'class_weight': None,\n",
       " 'fit_intercept': True,\n",
       " 'penalty': 'l1',\n",
       " 'random_state': 123}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvresults = pd.DataFrame(gs.cv_results_)[[\n",
    " 'mean_test_score',\n",
    " 'mean_train_score',\n",
    " 'param_C',\n",
    " 'param_penalty',\n",
    " 'param_fit_intercept',\n",
    " 'param_class_weight',\n",
    " 'std_test_score',\n",
    " 'std_train_score']]\n",
    "ha = list(cvresults)\n",
    "ha = [w.replace('param_', '') for w in ha]\n",
    "cvresults.columns = ha\n",
    "cvresults.to_excel(pathout + 'Modelo ' + nomemodelo + ' Resultados GridSearch.xlsx', \n",
    "                   encoding = 'latin1', index = False, engine = 'openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>fit_intercept</th>\n",
       "      <th>class_weight</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.939906</td>\n",
       "      <td>0.945907</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "      <td>True</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.009230</td>\n",
       "      <td>0.007218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.939960</td>\n",
       "      <td>0.945920</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>0.009151</td>\n",
       "      <td>0.007223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.952734</td>\n",
       "      <td>0.957796</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l1</td>\n",
       "      <td>True</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.014742</td>\n",
       "      <td>0.006431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.952774</td>\n",
       "      <td>0.957843</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l1</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>0.014820</td>\n",
       "      <td>0.006473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.971898</td>\n",
       "      <td>0.976743</td>\n",
       "      <td>1</td>\n",
       "      <td>l1</td>\n",
       "      <td>True</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.012034</td>\n",
       "      <td>0.004140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.971952</td>\n",
       "      <td>0.976827</td>\n",
       "      <td>1</td>\n",
       "      <td>l1</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>0.012015</td>\n",
       "      <td>0.004120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score  mean_train_score     C penalty fit_intercept class_weight  \\\n",
       "0         0.939906          0.945907  0.01      l1          True     balanced   \n",
       "1         0.939960          0.945920  0.01      l1          True         None   \n",
       "2         0.952734          0.957796   0.1      l1          True     balanced   \n",
       "3         0.952774          0.957843   0.1      l1          True         None   \n",
       "4         0.971898          0.976743     1      l1          True     balanced   \n",
       "5         0.971952          0.976827     1      l1          True         None   \n",
       "\n",
       "   std_test_score  std_train_score  \n",
       "0        0.009230         0.007218  \n",
       "1        0.009151         0.007223  \n",
       "2        0.014742         0.006431  \n",
       "3        0.014820         0.006473  \n",
       "4        0.012034         0.004140  \n",
       "5        0.012015         0.004120  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvresults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     mfcc1     mfcc2     mfcc3     mfcc4     mfcc5     mfcc6     mfcc7  \\\n",
      "0  1.22474 -0.340621  0.174583 -0.422942 -0.162481 -0.102888 -0.168227   \n",
      "\n",
      "      mfcc8     mfcc9    mfcc10    mfcc11    mfcc12    mfcc13  \n",
      "0  0.215698  0.134096  0.067047 -0.038332  0.153883 -0.361664  \n"
     ]
    }
   ],
   "source": [
    "def Feature_Importance(mod, nomemodelo, X, y):\n",
    "    many = ['Linear SVM', 'Logistica', 'Naive Bayes']\n",
    "\n",
    "    k = mod.best_estimator_\n",
    "    featurenames = list(X)\n",
    "    \n",
    "    if nomemodelo in many:\n",
    "        #Logistica, SVM, Naive Bayes\n",
    "        Features = pd.DataFrame(k.coef_.tolist())\n",
    "        Features.columns = featurenames\n",
    "        if len(y.unique()) > 2:\n",
    "            Features.index = sorted(y.unique())\n",
    "    else:\n",
    "        #Arvores\n",
    "        Features = pd.DataFrame({'features': featurenames, 'value': k.feature_importances_.tolist()})\n",
    "    \n",
    "    print(Features)\n",
    "    Features.to_excel(pathout + 'Features ' + nomemodelo + '.xlsx', encoding = 'latin1', index = True)\n",
    "    \n",
    "Feature_Importance(mod = gs, nomemodelo = nomemodelo, X = train[varsmod], y = train[y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preditos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predtr = gs.best_estimator_.predict(train[varsmod])\n",
    "predte = gs.best_estimator_.predict(teste[varsmod])\n",
    "\n",
    "predtrP = pd.DataFrame(gs.best_estimator_.predict_proba(train[varsmod]))\n",
    "predteP = pd.DataFrame(gs.best_estimator_.predict_proba(teste[varsmod]))\n",
    "\n",
    "predtrP = predtrP[predtrP.columns[1]]\n",
    "predteP = predteP[predteP.columns[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auc\n",
      "0.9788196715436375\n",
      "\n",
      "Acurácia\n",
      "0.928042328042328\n",
      "\n",
      "F1\n",
      "0.9282700421940928\n",
      "\n",
      "Precision\n",
      "0.9147609147609148\n",
      "\n",
      "Recall\n",
      "0.9421841541755889\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Auc')\n",
    "print(roc_auc_score(y_true = train[y], y_score = predtrP))\n",
    "print()\n",
    "print('Acurácia')\n",
    "print(accuracy_score(y_true = train[y], y_pred = predtr))\n",
    "print()\n",
    "print('F1')\n",
    "print(f1_score(y_true = train[y], y_pred = predtr))\n",
    "print()\n",
    "print('Precision')\n",
    "print(precision_score(y_true = train[y], y_pred = predtr))\n",
    "print()\n",
    "print('Recall')\n",
    "print(recall_score(y_true = train[y], y_pred = predtr))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auc\n",
      "0.7462504359958144\n",
      "\n",
      "Acurácia\n",
      "0.6990740740740741\n",
      "\n",
      "F1\n",
      "0.6632124352331606\n",
      "\n",
      "Precision\n",
      "0.6464646464646465\n",
      "\n",
      "Recall\n",
      "0.6808510638297872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Auc')\n",
    "print(roc_auc_score(y_true = teste[y], y_score = predteP))\n",
    "print()\n",
    "print('Acurácia')\n",
    "print(accuracy_score(y_true = teste[y], y_pred = predte))\n",
    "print()\n",
    "print('F1')\n",
    "print(f1_score(y_true = teste[y], y_pred = predte))\n",
    "print()\n",
    "print('Precision')\n",
    "print(precision_score(y_true = teste[y], y_pred = predte))\n",
    "print()\n",
    "print('Recall')\n",
    "print(recall_score(y_true = teste[y], y_pred = predte))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laran\\Anaconda3\\envs\\transc\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\laran\\Anaconda3\\envs\\transc\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\laran\\Anaconda3\\envs\\transc\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\laran\\Anaconda3\\envs\\transc\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "name = 'Treino'\n",
    "fim = train[[id, id2, y]]\n",
    "fim['pred'] = predtr\n",
    "\n",
    "fim['Acertou'] = np.where(fim[y] == fim['pred'], 1, 0)\n",
    "fim.to_excel(pathout + 'Modelo ' + nomemodelo + ' Pred ' + name + '.xlsx', encoding = 'latin1', index = False)\n",
    "\n",
    "del fim\n",
    "\n",
    "name = 'Teste'\n",
    "fim = teste[[id, id2, y]]\n",
    "fim['pred'] = predte\n",
    "\n",
    "fim['Acertou'] = np.where(fim[y] == fim['pred'], 1, 0)\n",
    "fim.to_excel(pathout + 'Modelo ' + nomemodelo + ' Pred ' + name + '.xlsx', encoding = 'latin1', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env transc",
   "language": "python",
   "name": "transc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
